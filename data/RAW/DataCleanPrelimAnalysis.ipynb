{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import missingno as msno\n",
    "# from pandas_profiling import ProfileReport\n",
    "# https://stackoverflow.com/questions/64137932/create-interaction-term-in-scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OverSampler(parentDIR,df, xfilename, yfilename):\n",
    "    # Oversample small class to balanced data\n",
    "    os.chdir(parentDIR)\n",
    "    df = df.dropna(how='all')\n",
    "    # X = df.iloc[:,4:].copy()\n",
    "    X = df.drop(['index','PatientID', 'TimeGroup','SerumID', 'Status', 'Rep'], axis=1).copy().reset_index(drop=True)\n",
    "    # X['TimeGroup'] = df.TimeGroup.copy()\n",
    "    X = pd.get_dummies(X)\n",
    "    Y = df.Status.copy()\n",
    "\n",
    "    ros = RandomOverSampler(random_state=0)\n",
    "    Xoversampled, Yoversampled = ros.fit_resample(X, Y)\n",
    "\n",
    "    # if not os.path.isdir('OverSampled'):\n",
    "    #     os.makedirs('OverSampled')\n",
    "    # os.chdir('OverSampled')\n",
    "\n",
    "    Xoversampled.to_csv(xfilename, sep='\\t')\n",
    "    Yoversampled.to_csv(yfilename, sep='\\t')\n",
    "    return Xoversampled, Yoversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parentdir = os.getcwd()\n",
    "testdata = pd.read_excel('Human Training and Testing sets for model.xlsx', sheet_name='Human Testing')\n",
    "traindata = pd.read_excel('Human Training and Testing sets for model.xlsx', sheet_name='Human Training')\n",
    "Full = pd.concat([testdata, traindata ]).reset_index()\n",
    "Full.to_csv('Concatenated_RAW.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full.isna().sum()\n",
    "Full.dropna(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grouping Data\n",
    "* finding average values from technical replicates. (Decided to use whole data and not average). \n",
    "* dropping whole cell lysate as it is not informative to antigens present for Burk infection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IgG\n",
      "Negative    1436\n",
      "Melioid     1428\n",
      "Name: Status, dtype: int64\n",
      "IgM\n",
      "Negative    1436\n",
      "Melioid     1428\n",
      "Name: Status, dtype: int64\n",
      "IgM\n",
      "Healthy    1436\n",
      "Week4+      963\n",
      "Week2       180\n",
      "Week3       163\n",
      "Week1       106\n",
      "Prior        16\n",
      "Name: TimeGroup, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# group = Full.groupby(['PatientID', 'SerumID', 'TimeGroup', 'Status', 'Type']).mean().reset_index()\n",
    "Clean = Full.drop(['MSHR5855.WCL'], axis=1)\n",
    "for name, group in Clean.groupby(\"Type\"):\n",
    "    print(name)\n",
    "    print(Clean.Status.value_counts())\n",
    "\n",
    "for name, group in group.groupby(\"Type\"):\n",
    "    print(name)\n",
    "    print(Clean.TimeGroup.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NonResampled Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('NonResampled'):\n",
    "    os.makedirs('NonResampled')\n",
    "os.chdir('NonResampled')\n",
    "NonresDir = os.getcwd()\n",
    "\n",
    "for name, group in Clean.groupby(\"Type\"): # add SerumID to group\n",
    "    print(name)\n",
    "    os.chdir(NonresDir)\n",
    "    if not os.path.isdir(name):\n",
    "        os.makedirs(name)\n",
    "    os.chdir(name)\n",
    "    TypeDir = os.getcwd()\n",
    "    if not os.path.isdir('ValidationSet'):\n",
    "        os.makedirs('ValidationSet')\n",
    "    os.chdir('ValidationSet')\n",
    "    \n",
    "    Week4 = group.loc[(group['TimeGroup']=='Week4+')].copy().sample(frac=0.1, random_state=0)\n",
    "    Week3 = group.loc[(group['TimeGroup']=='Week3')].copy().sample(frac=0.1, random_state=0)\n",
    "    Week2 = group.loc[(group['TimeGroup']=='Week2')].copy().sample(frac=0.1, random_state=0)\n",
    "    Week1 = group.loc[(group['TimeGroup']=='Week1')].copy().sample(frac=0.1, random_state=0)\n",
    "    prior = group.loc[(group['TimeGroup']=='Prior')].copy().sample(frac=0.1, random_state=0)\n",
    "    Negative = group.loc[(group['TimeGroup']=='Healthy')].copy().sample(frac=0.1, random_state=0)\n",
    "\n",
    "    ValidationSet = pd.concat([Week1, Week2, Week3, Week4, prior, Negative])\n",
    "    ValidationSet.to_csv('ValidationSet.txt', sep='\\t')\n",
    "    print(ValidationSet.TimeGroup.value_counts())\n",
    "\n",
    "    group.drop(Week1.index, inplace=True)\n",
    "    group.drop(Week2.index, inplace=True)\n",
    "    group.drop(Week3.index, inplace=True)\n",
    "    group.drop(Week4.index, inplace=True)\n",
    "    group.drop(Negative.index, inplace=True)\n",
    "    group.drop(prior.index, inplace=True)\n",
    "\n",
    "\n",
    "    os.chdir(TypeDir)\n",
    "    Week4 = group.loc[(group['TimeGroup']=='Week4+')|(group['TimeGroup']=='Healthy')].copy()\n",
    "    Week3 = group.loc[(group['TimeGroup']=='Week3')|(group['TimeGroup']=='Healthy')].copy()\n",
    "    Week2 = group.loc[(group['TimeGroup']=='Week2')|(group['TimeGroup']=='Healthy')].copy()\n",
    "    Week1 = group.loc[(group['TimeGroup']=='Week1')|(group['TimeGroup']=='Healthy')].copy()\n",
    "    Prior = group.loc[(group['TimeGroup']=='Prior')|(group['TimeGroup']=='Healthy')].copy()\n",
    "    Week4.to_csv('Week4_'+str(name)+'_PosNeg_grouped.txt', sep='\\t')\n",
    "    Week3.to_csv('Week3_'+str(name)+'_PosNeg_grouped.txt', sep='\\t')\n",
    "    Week2.to_csv('Week2_'+str(name)+'_PosNeg_grouped.txt', sep='\\t')\n",
    "    Week1.to_csv('Week1_'+str(name)+'_PosNeg_grouped.txt', sep='\\t')\n",
    "    Prior.to_csv('Prior_'+str(name)+'_PosNeg_grouped.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(NonresDir)\n",
    "\n",
    "igg4 = pd.read_csv('IgG/Week4_IgG_PosNeg_grouped.txt')\n",
    "igg3 = pd.read_csv('IgG/Week3_IgG_PosNeg_grouped.txt')\n",
    "igg2 = pd.read_csv('IgG/Week2_IgG_PosNeg_grouped.txt')\n",
    "igg1 = pd.read_csv('IgG/Week1_IgG_PosNeg_grouped.txt')\n",
    "\n",
    "igm4 = pd.read_csv('IgM/Week4_IgM_PosNeg_grouped.txt')\n",
    "igm3 = pd.read_csv('IgM/Week3_IgM_PosNeg_grouped.txt')\n",
    "igm2 = pd.read_csv('IgM/Week2_IgM_PosNeg_grouped.txt')\n",
    "igm1 = pd.read_csv('IgM/Week1_IgM_PosNeg_grouped.txt')\n",
    "\n",
    "if not os.path.isdir('IgGM'):\n",
    "    os.makedirs('IgGM')\n",
    "os.chdir('IgGM')\n",
    "iggm1 = pd.concat([igg1,igm1])\n",
    "iggm2 = pd.concat([igg2,igm2])\n",
    "iggm3 = pd.concat([igg3,igm3])\n",
    "iggm4 = pd.concat([igg4,igm4])\n",
    "\n",
    "iggm1.to_csv(\"Week1_IgGM_PosNeg_grouped.txt\", sep='\\t')\n",
    "iggm2.to_csv(\"Week2_IgGM_PosNeg_grouped.txt\", sep='\\t')\n",
    "iggm3.to_csv(\"Week3_IgGM_PosNeg_grouped.txt\", sep='\\t')\n",
    "iggm4.to_csv(\"Week4_IgGM_PosNeg_grouped.txt\", sep='\\t')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OverSampled Data Cleaning and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IgG\n",
      "Negative    646\n",
      "Melioid     435\n",
      "Name: Status, dtype: int64\n",
      "IgM\n",
      "Negative    646\n",
      "Melioid     432\n",
      "Name: Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "os.chdir(parentdir)\n",
    "if not os.path.isdir('OverSampled'):\n",
    "    os.makedirs('OverSampled')\n",
    "os.chdir('OverSampled')\n",
    "OverDir = os.getcwd()\n",
    "\n",
    "for name, group in Clean.groupby(\"Type\"): # add SerumID to group\n",
    "    print(name)\n",
    "    os.chdir(OverDir)\n",
    "    if not os.path.isdir(name):\n",
    "        os.makedirs(name)\n",
    "    os.chdir(name)\n",
    "    TypeDir = os.getcwd()\n",
    "    if not os.path.isdir('ValidationSet'):\n",
    "        os.makedirs('ValidationSet')\n",
    "    os.chdir('ValidationSet')\n",
    "\n",
    "    Week4 = group.loc[(group['TimeGroup']=='Week4+')].copy().sample(frac=0.1, random_state=0)\n",
    "    Week3 = group.loc[(group['TimeGroup']=='Week3')].copy().sample(frac=0.1, random_state=0)\n",
    "    Week2 = group.loc[(group['TimeGroup']=='Week2')].copy().sample(frac=0.1, random_state=0)\n",
    "    Week1 = group.loc[(group['TimeGroup']=='Week1')].copy().sample(frac=0.1, random_state=0)\n",
    "    prior = group.loc[(group['TimeGroup']=='Prior')].copy().sample(frac=0.1, random_state=0)\n",
    "    Negative = group.loc[(group['TimeGroup']=='Healthy')].copy().sample(frac=0.1, random_state=0)\n",
    "\n",
    "    x1, y1 = OverSampler(os.getcwd(),pd.concat([Week1,Negative]), 'week1_'+str(name)+'predictor.txt', 'week1_'+str(name)+'response.txt')\n",
    "    x2, y2 = OverSampler(os.getcwd(),pd.concat([Week2,Negative]), 'week2_'+str(name)+'predictor.txt', 'week2_'+str(name)+'response.txt')\n",
    "    x3, y3 = OverSampler(os.getcwd(),pd.concat([Week3,Negative]), 'week3_'+str(name)+'predictor.txt', 'week3_'+str(name)+'response.txt')\n",
    "    x4, y4 = OverSampler(os.getcwd(),pd.concat([Week4,Negative]), 'week4_'+str(name)+'predictor.txt', 'week4_'+str(name)+'response.txt')\n",
    "\n",
    "    # ValidationSet = pd.concat([Week1, Week2, Week3, Week4, prior, Negative])\n",
    "    # ValidationSet.to_csv('ValidationSet.txt', sep='\\t')\n",
    "    # print(ValidationSet.TimeGroup.value_counts())\n",
    "\n",
    "\n",
    "    group.drop(Week1.index, inplace=True)\n",
    "    group.drop(Week2.index, inplace=True)\n",
    "    group.drop(Week3.index, inplace=True)\n",
    "    group.drop(Week4.index, inplace=True)\n",
    "    group.drop(Negative.index, inplace=True)\n",
    "    group.drop(prior.index, inplace=True)\n",
    "\n",
    "\n",
    "    os.chdir(TypeDir)\n",
    "    Week4 = group.loc[(group['TimeGroup']=='Week4+')|(group['TimeGroup']=='Healthy')].copy()\n",
    "    Week3 = group.loc[(group['TimeGroup']=='Week3')|(group['TimeGroup']=='Healthy')].copy()\n",
    "    Week2 = group.loc[(group['TimeGroup']=='Week2')|(group['TimeGroup']=='Healthy')].copy()\n",
    "    Week1 = group.loc[(group['TimeGroup']=='Week1')|(group['TimeGroup']=='Healthy')].copy()\n",
    "    Prior = group.loc[(group['TimeGroup']=='Prior')|(group['TimeGroup']=='Healthy')].copy()\n",
    "\n",
    "    print(Week4.Status.value_counts())\n",
    "    \n",
    "    X4, Y4 = OverSampler(TypeDir, Week4, 'W4_predictor.txt', 'W4_response.txt')\n",
    "    X3, Y3 = OverSampler(TypeDir, Week3, 'W3_predictor.txt', 'W3_response.txt')\n",
    "    X2, Y2 = OverSampler(TypeDir, Week2, 'W2_predictor.txt', 'W2_response.txt')\n",
    "    X1, Y1 = OverSampler(TypeDir, Week1, 'W1_predictor.txt', 'W1_response.txt')\n",
    "\n",
    "\n",
    "    # Week4.to_csv('Week4_'+str(name)+'_PosNeg_grouped.txt', sep='\\t')\n",
    "    # Week3.to_csv('Week3_'+str(name)+'_PosNeg_grouped.txt', sep='\\t')\n",
    "    # Week2.to_csv('Week2_'+str(name)+'_PosNeg_grouped.txt', sep='\\t')\n",
    "    # Week1.to_csv('Week1_'+str(name)+'_PosNeg_grouped.txt', sep='\\t')\n",
    "    # Prior.to_csv('Prior_'+str(name)+'_PosNeg_grouped.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(OverDir)\n",
    "\n",
    "igg4p = pd.read_csv('IgG/W4_predictor.txt')\n",
    "igg3p = pd.read_csv('IgG/W3_predictor.txt')\n",
    "igg2p = pd.read_csv('IgG/W2_predictor.txt')\n",
    "igg1p = pd.read_csv('IgG/W1_predictor.txt')\n",
    "\n",
    "igg4r = pd.read_csv('IgG/W4_response.txt')\n",
    "igg3r = pd.read_csv('IgG/W3_response.txt')\n",
    "igg2r = pd.read_csv('IgG/W2_response.txt')\n",
    "igg1r = pd.read_csv('IgG/W1_response.txt')\n",
    "\n",
    "igm4p = pd.read_csv('IgM/W4_predictor.txt')\n",
    "igm3p = pd.read_csv('IgM/W3_predictor.txt')\n",
    "igm2p = pd.read_csv('IgM/W2_predictor.txt')\n",
    "igm1p = pd.read_csv('IgM/W1_predictor.txt')\n",
    "\n",
    "igm4r = pd.read_csv('IgM/W4_response.txt')\n",
    "igm3r = pd.read_csv('IgM/W3_response.txt')\n",
    "igm2r = pd.read_csv('IgM/W2_response.txt')\n",
    "igm1r = pd.read_csv('IgM/W1_response.txt')\n",
    "\n",
    "if not os.path.isdir('IgGM'):\n",
    "    os.makedirs('IgGM')\n",
    "os.chdir('IgGM')\n",
    "iggm1p = pd.concat([igg1p,igm1p])\n",
    "iggm1r = pd.concat([igg1r,igm1r])\n",
    "iggm2p = pd.concat([igg2p,igm2p])\n",
    "iggm2r = pd.concat([igg2r,igm2r])\n",
    "iggm3p = pd.concat([igg3p,igm3p])\n",
    "iggm3r = pd.concat([igg3r,igm3r])\n",
    "iggm4p = pd.concat([igg4p,igm4p])\n",
    "iggm4r = pd.concat([igg4r,igm4r])\n",
    "\n",
    "iggm1p.to_csv(\"W1_predictor.txt\", sep='\\t')\n",
    "iggm1r.to_csv(\"W1_response.txt\", sep='\\t')\n",
    "iggm2p.to_csv(\"W2_predictor.txt\", sep='\\t')\n",
    "iggm2r.to_csv(\"W2_response.txt\", sep='\\t')\n",
    "iggm3p.to_csv(\"W3_predictor.txt\", sep='\\t')\n",
    "iggm3r.to_csv(\"W3_response.txt\", sep='\\t')\n",
    "iggm4p.to_csv(\"W4_predictor.txt\", sep='\\t')\n",
    "iggm4r.to_csv(\"W4_response.txt\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clean.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clean.columns\n",
    "Clean_min = ['BPSL1201_IMPS',\n",
    "       'BPSL1404_ClpX', 'BPSL1743_Arg', 'BPSL2096_AhpC', 'BPSL2522_OmpA',\n",
    "       'BPSL2697_GroEL', 'BPSL2827_DNAK', 'BPSL3222_rpIL', 'BPSL3396_AtpD',\n",
    "       'BPSS0135', 'BPSS0476_GroS', 'BPSS0477_GroEL2', 'BPSS0530',\n",
    "       'BPSS1498_HCP1.B', 'BPSS1652', 'BPSS1769_NADH', 'BPSS1850', 'CPS',\n",
    "       'LPSA', 'LPSB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette('colorblind')\n",
    "for col in Clean_min:\n",
    "    fig,ax = plt.subplots()\n",
    "    sns.boxplot(data=Clean, x=col, y='TimeGroup', ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(group, hue=\"Status\", diag_kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(group, hue=\"TimeGroup\", diag_kind=\"hist\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BurkPx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9cf4abd829db67bd613b172b6a5aacbce0ad65f853b713fd5bab49069a3ec93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
